{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M_eJxepJjMU"
      },
      "source": [
        "We first get necessary external data and code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhfkEfaoA9eT",
        "outputId": "f72d6bd2-e387-41e4-c951-a37a0593e225"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deepinterpolation' already exists and is not an empty directory.\n",
            "�T�u�f�B���N�g���܂��̓t�@�C�� -p �͊��ɑ��݂��܂��B\n",
            "�������ɃG���[���������܂���: -p\n",
            "�T�u�f�B���N�g���܂��̓t�@�C�� ephys �͊��ɑ��݂��܂��B\n",
            "�������ɃG���[���������܂���: ephys\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AllenInstitute/deepinterpolation.git\n",
        "!mkdir -p ephys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjcbfWj_Jopq"
      },
      "source": [
        "Install deepinterpolation package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK8RFhYQJoyK",
        "outputId": "2db29cec-384d-421c-966d-e54aa0ffb772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/AllenInstitute/deepinterpolation.git\n",
            "  Cloning https://github.com/AllenInstitute/deepinterpolation.git to c:\\users\\hyperpc_smee\\appdata\\local\\temp\\pip-req-build-lkgsf4ka\n",
            "  Resolved https://github.com/AllenInstitute/deepinterpolation.git to commit 8057088a0d6b4b1889014ffaa2f107fc2d88896a\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: tensorflow in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (2.4.4)\n",
            "Requirement already satisfied: nibabel in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (5.1.0)\n",
            "Requirement already satisfied: h5py in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (3.6.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (1.10.1)\n",
            "Requirement already satisfied: tifffile in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (2023.7.10)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from deepinterpolation==0.2.0) (4.66.5)\n",
            "Requirement already satisfied: six in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from h5py->deepinterpolation==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from matplotlib->deepinterpolation==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from nibabel->deepinterpolation==0.2.0) (6.4.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (0.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (3.20.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (0.44.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorflow->deepinterpolation==0.2.0) (1.32.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tqdm->deepinterpolation==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from importlib-resources>=1.3->nibabel->deepinterpolation==0.2.0) (3.20.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (2.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (75.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hyperpc_smee\\anaconda3\\envs\\dip38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->deepinterpolation==0.2.0) (3.2.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/AllenInstitute/deepinterpolation.git 'C:\\Users\\HyperPC_Smee\\AppData\\Local\\Temp\\pip-req-build-lkgsf4ka'\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/AllenInstitute/deepinterpolation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "D8RG4iRoCRUE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from deepinterpolation.generator_collection import SingleTifGenerator\n",
        "from deepinterpolation.trainor_collection import core_trainer\n",
        "from deepinterpolation.network_collection import unet_single_1024\n",
        "import datetime\n",
        "\n",
        "import tkinter.filedialog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nogrv6xClZQ"
      },
      "source": [
        "This is used for record-keeping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lwuBrQ_jC6Ya"
      },
      "outputs": [],
      "source": [
        "now = datetime.datetime.now()\n",
        "run_uid = now.strftime(\"%Y_%m_%d_%H_%M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPO8knNmC62U"
      },
      "source": [
        "Initialize meta-parameters objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5HMK5cRDC-gz"
      },
      "outputs": [],
      "source": [
        "training_param = {}\n",
        "generator_param = {}\n",
        "network_param = {}\n",
        "generator_test_param = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn7PSaI-C-01"
      },
      "source": [
        "An epoch is defined as the number of batches pulled from the dataset. Because our datasets are VERY large. Often, we cannot\n",
        "go through the entirity of the data so we define an epoch slightly differently than is usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CzKVMosLDCsb"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = 10\n",
        "pre_post_frame = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7s66NK5DCx_"
      },
      "source": [
        "Those are parameters used for the Validation test generator. Here the test is done on the beginning of the data but\n",
        "this can be a separate file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Those are parameters used for the Validation test generator. Here the\n",
        "# test is done on the beginning of the data but\n",
        "# this can be a separate file\n",
        "generator_test_param[\n",
        "    \"pre_post_frame\"\n",
        "] = pre_post_frame  # Number of frame provided before and after the predicted frame\n",
        "filePath = tkinter.filedialog.askopenfilename()\n",
        "generator_test_param[\"train_path\"] = filePath\n",
        "generator_test_param[\"batch_size\"] = 5\n",
        "generator_test_param[\"start_frame\"] = 0\n",
        "generator_test_param[\"end_frame\"] = 99\n",
        "generator_test_param[\n",
        "    \"pre_post_omission\"\n",
        "] = 1  # Number of frame omitted before and after the predicted frame\n",
        "generator_test_param[\n",
        "    \"steps_per_epoch\"\n",
        "] = -1\n",
        "# No step necessary for testing as epochs are not relevant.\n",
        "# -1 deactivate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3VsVLdJEC-8k"
      },
      "outputs": [],
      "source": [
        "generator_param[\"steps_per_epoch\"] = steps_per_epoch\n",
        "generator_param[\n",
        "    \"pre_post_frame\"\n",
        "] = generator_test_param[\"pre_post_frame\"]  # Number of frame provided before and after the predicted frame\n",
        "# In practice the test and validation would point at different files or different portion of files. But for demo, we have little data available\n",
        "\n",
        "generator_param[\"train_path\"] = filePath\n",
        "generator_param[\"batch_size\"] = 1\n",
        "generator_param[\"start_frame\"] = 0\n",
        "generator_param[\"end_frame\"] = 99\n",
        "generator_param[\n",
        "    \"pre_post_omission\"\n",
        "] = 0  # Number of frame omitted before and after the predicted frame\n",
        "generator_param[\"steps_per_epoch\"] = -1  # No step necessary for testing as epochs are not relevant. -1 deactivate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7e0BhTBEH_L"
      },
      "source": [
        "Those are parameters used for the main data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'D:/deepinterpolation/sample_data/C2-reged_651-08.0w-D1-xyt6-300.tif'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator_param[\"train_path\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvS5Y1yERiY"
      },
      "source": [
        "Those are parameters used for the network topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAu-UN8zEjxz"
      },
      "source": [
        "Those are parameters used for the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VjINOf20EkCz"
      },
      "outputs": [],
      "source": [
        "training_param[\"run_uid\"] = run_uid\n",
        "training_param[\"batch_size\"] = generator_test_param[\"batch_size\"]\n",
        "training_param[\"steps_per_epoch\"] = steps_per_epoch\n",
        "training_param[\n",
        "    \"period_save\"\n",
        "] = 25  # network model is potentially saved during training between a regular nb epochs\n",
        "training_param[\"nb_gpus\"] = 0\n",
        "training_param[\"apply_learning_decay\"] = 0\n",
        "training_param[\n",
        "    \"nb_times_through_data\"\n",
        "] = 1  # if you want to cycle through the entire data. Two many iterations will cause noise overfitting\n",
        "training_param[\"learning_rate\"] = 0.0001\n",
        "training_param[\"pre_post_frame\"] = generator_test_param[\"pre_post_frame\"]\n",
        "training_param[\"loss\"] = \"mean_absolute_error\"\n",
        "training_param[\n",
        "    \"nb_workers\"\n",
        "] = 1  # this is to enable multiple threads for data generator loading. Useful when this is slower than training\n",
        "\n",
        "training_param[\"model_string\"] = (\n",
        "    \"unet_single_1024_\"\n",
        "    + training_param[\"loss\"]\n",
        "    + \"_\"\n",
        "    + training_param[\"run_uid\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STXWqSIHEkSr"
      },
      "source": [
        "Where do you store ongoing training progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3a_PwgRGEkjx"
      },
      "outputs": [],
      "source": [
        "jobdir = os.path.join(\n",
        "    training_param[\"model_string\"] + \"_\" + run_uid,\n",
        ")\n",
        "training_param[\"output_dir\"] = jobdir\n",
        "\n",
        "try:\n",
        "    os.mkdir(jobdir)\n",
        "except:\n",
        "    print(\"folder already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'unet_single_1024_mean_absolute_error_2024_10_22_16_49_2024_10_22_16_49'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_param[\"output_dir\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtk6kyuYE0qC"
      },
      "source": [
        "Here we create all json files that are fed to the training. This is used for recording purposes as well as input to the training proces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5O5T6EmE7ss"
      },
      "source": [
        "Here we create all objects for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrOO6Pc7E71m",
        "outputId": "44c55219-8b15-43f9-ebdf-23efb0833b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "# We build the generators object. This will, among other things, calculate normalizing parameters.\n",
        "train_generator = SingleTifGenerator(generator_param)\n",
        "test_generator = SingleTifGenerator(generator_test_param)\n",
        "\n",
        "# We build the training object.\n",
        "training_class = core_trainer(train_generator, test_generator, unet_single_1024({}), training_param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpVKYBcsCCuz"
      },
      "source": [
        "Start training. This can take very long time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hw-IUVbAxSw",
        "outputId": "0544b182-caa1-493a-f9e3-c5080375c02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 13s 665ms/step - loss: 1.3805 - val_loss: 0.3340\n",
            "Epoch 2/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 6s 65ms/step - loss: 0.3455 - val_loss: 0.3278\n",
            "Epoch 3/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 5s 51ms/step - loss: 0.3373 - val_loss: 0.3221\n",
            "Epoch 4/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 5s 52ms/step - loss: 0.3288 - val_loss: 0.3132\n",
            "Epoch 5/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 5s 51ms/step - loss: 0.3181 - val_loss: 0.3039\n",
            "Epoch 6/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 6s 66ms/step - loss: 0.3104 - val_loss: 0.2999\n",
            "Epoch 7/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 6s 66ms/step - loss: 0.3071 - val_loss: 0.2957\n",
            "Epoch 8/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 6s 69ms/step - loss: 0.3042 - val_loss: 0.2939\n",
            "Epoch 9/9\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "10/10 [==============================] - 6s 64ms/step - loss: 0.3020 - val_loss: 0.2920\n"
          ]
        }
      ],
      "source": [
        "training_class.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FedXESQAB7f7"
      },
      "source": [
        "Finalize and save output of the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGgJLL5hA0N8",
        "outputId": "4b571095-942c-4300-8253-e655fedb064a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "training_class.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'run_uid': '2024_10_11_17_44',\n",
              " 'batch_size': 1,\n",
              " 'steps_per_epoch': 5,\n",
              " 'period_save': 25,\n",
              " 'nb_gpus': 0,\n",
              " 'apply_learning_decay': 0,\n",
              " 'nb_times_through_data': 1,\n",
              " 'learning_rate': 0.0001,\n",
              " 'pre_post_frame': 30,\n",
              " 'loss': 'mean_absolute_error',\n",
              " 'nb_workers': 1,\n",
              " 'model_string': 'unet_single_1024_mean_absolute_error_2024_10_11_17_44',\n",
              " 'output_dir': 'unet_single_1024_mean_absolute_error_2024_10_11_17_44_2024_10_11_17_44'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_param"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "example_tiny_ephys_training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "DIP38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
